{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP8u8/833E4vDbBZ0nWMESx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ERIC10000/introduction-to-large-language-models/blob/main/Large_Language_Model_Introduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ZNsK5NH0leXH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# History of Large Language Models (LLMs)\n",
        "In recent years, large language models (LLMs) have emerged as transformative tools in the field of natural\n",
        "language processing (NLP). Powered by advanced machine learning techniques, these models have revolutionized the way we interact with and understand textual data. In this comprehensive guide, we'll delve\n",
        "into the inner workings of LLMs, explore their capabilities, and discuss their vast potential in various applications, all with a focus on Python development.\n",
        "\n",
        "# Understanding Large Language Models.\n",
        "At the core of LLMs lies the concept of deep learning, a subfield of machine learning that focuses on\n",
        "training neural networks with multiple layers to extract complex patterns from data. In the context of\n",
        "NLP, LLMs are neural network architectures designed to understand and generate human-like text.\n",
        "One of the most prominent LLM architectures is OpenAI's GPT (Generative Pre-trained Transformer)\n",
        "model. Let's take a look at how we can leverage the power of GPT-3, the third iteration of the GPT series,\n",
        "using Python code:\n",
        "\n",
        "\n",
        "# Applications of Large Language Models\n",
        "1.  <b>Content Generation:</b> LLMs excel at generating human-like text across various genres, including storytelling, news articles, and technical documentation, report writing etc\n",
        "\n",
        "2. <b>Language Translation:</b> LLMs can also facilitate language translation tasks by converting text from one\n",
        "language to another.\n",
        "\n",
        "3. <b>Sentiment Analysis: </b> LLMs can analyze the sentiment of textual data, helping businesses gain insights\n",
        "into customer feedback and social media interactions. With Sentiment Analysis, a machine can understand the context of a text and suggest whether the context is positive, negative or neutral.\n",
        "\n",
        "\n",
        "# Architecture of Large Language Models:\n",
        "At the heart of LLMs lies a complex neural network architecture, typically based on transformer models.\n",
        "These models consist of multiple layers of attention mechanisms, which enable them to capture long range dependencies within the input text. One ofthe most prominent examples of an LLM architecture is\n",
        "OpenAI's GPT (Generative Pre-trained Transformer) series."
      ],
      "metadata": {
        "id": "nUSyu6jDlu9C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading llm libraries\n",
        "!pip install transformers # Load the pre-trained models from hagging face as Generative Pre-Trained Transformers(GPT)\n",
        "!pip install tokenizers # Break down a user input into tokens, so that the input can be understood by the model.\n",
        "!pip install torch torchvision torchaudio # Buiding Block of Neural Networks built by facebook.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zcMoEwrFqUc6",
        "outputId": "14830d54-1349-4325-a9fa-430ef6cde3d6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (0.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers) (0.24.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2024.8.30)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.19.1+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (10.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Steps of Building Large Language Models\n",
        "1. Requesting the User Prompt\n",
        "We use a concept called <b>Prompt Engineering</b> that help users generate the right prompt that can be understood by a Large Language Models. Research on Prompt Engineering Techniques..."
      ],
      "metadata": {
        "id": "LvIwa1SjsngZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = str(input(\"Ask Anything: \"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yc9rcz7LtqI1",
        "outputId": "1a0d8e57-fbdd-4c38-d963-cfa754f33c49"
      },
      "execution_count": 10,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ask Anything: What is Artificial Intelligence?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Load the Pre-Trained Models from the Hagging Face AI Community using the transformers library depending on the context, e,g text-to-speech, sentiment-analysis, code-completion, text-summarization and text-generation."
      ],
      "metadata": {
        "id": "jiJo9XQ9uLXj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer,GPT2LMHeadModel\n",
        "from warnings import filterwarnings\n",
        "filterwarnings('ignore')\n",
        "\n",
        "# Load pre-trained GPT-2 model and tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')"
      ],
      "metadata": {
        "id": "wDl3ogQRuv9E"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Tokenization of the Prompt. Tokenization is the processing in NLP where a textual information is broken down into smaller token that can be used with an LLM Application. This process sometimes is called <b>Encoding</b> because it converts a text to numerical format called tensors."
      ],
      "metadata": {
        "id": "U0m6mUmhvbvX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = tokenizer.encode(prompt,return_tensors='pt')\n",
        "tokens\n",
        "# With the tokenizer, the user prompt is converted into smaller numerical tokens that we call tensors.\n",
        "# This is because machines and algorithms work better with numbers rather than text."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9tKPusDWv-7M",
        "outputId": "d543dd01-3c81-4e0a-d199-3b65d4da25de"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 2061,   318, 35941,  9345,    30]])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Text Generation(Decoding): This is the process where we decodes outputs from our model based on the user prompt using a pre-trained model."
      ],
      "metadata": {
        "id": "PXU-avjvxdsb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = model.generate(tokens,max_length=100,num_beams=5,no_repeat_ngram_size=2,early_stopping=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gGXQ1Mr3x_S5",
        "outputId": "d620b990-4a2b-4a43-e606-2c838bc1deff"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Decode  the Outputing.\n",
        "# This is the process of convert the outputs as Tensors(Numerical) into textual Representation.\n",
        "text = tokenizer.decode(output[0],skip_special_tokens=True)\n",
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "6DTQUQFUzI7Q",
        "outputId": "51613917-3d4b-4f17-fba9-cd9d1168b316"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'What is Artificial Intelligence?\\n\\nA lot of people think of artificial intelligence as something that\\'s going to happen in the next few years, but it\\'s not. Artificial intelligence is not a new concept. It\\'s been around for a long time, and there are many different ways to think about it. There are a number of different types of AI, some of which are called \"deep learning\" or \"machine learning,\" and many of them are very different from what we\\'re used to.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    }
  ]
}